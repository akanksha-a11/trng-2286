{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c459104b",
   "metadata": {},
   "source": [
    "# Activity\n",
    "path to file\n",
    "\n",
    "```\n",
    "/datasets/employee_work_records.csv\n",
    "```\n",
    "\n",
    "### Tasks\n",
    "\n",
    "1. Display the first 10 rows of the dataset to understand its schema and contents.\n",
    "2. Print the schema of the DataFrame and identify the data types of each column.\n",
    "3. Count how many rows contain at least one null value.\n",
    "4. Replace all null values in the hours_worked column with the average hours worked (calculated from non-null rows).\n",
    "5. Replace all the null values in location as Remote\n",
    "6. Drop all rows that have a null value in either salary or projects_completed.\n",
    "7. Add a new column named hourly_rate which is calculated as:\n",
    "  `salary / (hours_worked Ã— 4)`\n",
    "8. Group the data by department and calculate:\n",
    "  - Average salary\n",
    "  - Average hours_worked\n",
    "  - Total projects completed\n",
    "9. Group by location and count how many employees work in each location.\n",
    "10. Find the maximum salary and minimum salary\n",
    "11. Filter out employees who have completed more than 5 projects and worked less than 35 hours.\n",
    "12. Create a temporary view named employees.\n",
    "Using Spark SQL:\n",
    "  - Select department-wise employee count and average salary.\n",
    "  - Filter only departments where average salary is above 75,000.\n",
    "  - Write a SQL query to get count of whose salary is above the average salary.\n",
    "13. Create a column bonus_eligible where employees with projects_completed > 5 are marked \"Yes\", else \"No\".\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed3ecfb7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
