{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c576e44b",
   "metadata": {},
   "source": [
    "# Spark Optimization\n",
    "\n",
    "### Partitioning\n",
    "\n",
    "- Partitioning refers to dividing data into logical chunks (partitions) across nodes.\n",
    "- Effective partitioning improves parallelism, reduces shuffle, and enhances query performance.\n",
    "\n",
    "#### Partitioning in memory\n",
    "\n",
    "- Repartition\n",
    "    - allows to specify the desired number of partitions and the columns to partition by\n",
    "    - shuffles the data to create the specified number of partitions\n",
    "\n",
    "- Coalesce\n",
    "    - reduces the number of partitions by merging them\n",
    "    - useful when you want to decrease the number of partitions for efficiency\n",
    "\n",
    "#### Partitioning on disk\n",
    "\n",
    "\n",
    "- `partitionBy()` method is used to partition the data into a file system, resulting in multiple sub-directories.\n",
    "- this enhances the read performance for downstream systems.\n",
    "- This function can be applied to one or multiple column values while writing a DataFrame to the disk.\n",
    "\n",
    "\n",
    "\n",
    "[spark performace tuning](https://spark.apache.org/docs/latest/sql-performance-tuning.html)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a97aea85",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "25/08/07 03:49:47 WARN Utils: Your hostname, codespaces-c6070e, resolves to a loopback address: 127.0.0.1; using 10.0.0.91 instead (on interface eth0)\n",
      "25/08/07 03:49:47 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/08/07 03:49:50 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"CustomerOrdersDemo\").getOrCreate()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cc7a422f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+----------+----------+------+\n",
      "|            order_id|         customer_id|order_date|product_id|amount|\n",
      "+--------------------+--------------------+----------+----------+------+\n",
      "|02a777e0-5571-42c...|0e99a07c-c7a5-43d...|2023-04-21|     P1031|375.94|\n",
      "|1c5a3e4d-f8de-47b...|3a69ac3e-6726-431...|2021-09-25|     P1086|373.51|\n",
      "|a5b65d4d-3ac0-45d...|3a69ac3e-6726-431...|2024-01-04|     P1054| 61.73|\n",
      "|b752df2c-aa68-41e...|c63cab5f-dc06-484...|2024-01-16|     P1029| 64.97|\n",
      "|23e8adb9-330d-4ce...|50b165d0-6486-4d5...|2021-10-27|     P1091| 289.4|\n",
      "|f76db88b-9100-4b0...|50b165d0-6486-4d5...|2024-09-23|     P1057|221.37|\n",
      "|3bb7142d-b348-486...|50b165d0-6486-4d5...|2023-03-26|     P1000|408.53|\n",
      "|b4002908-2ad1-4d7...|50b165d0-6486-4d5...|2022-07-26|     P1043|355.11|\n",
      "|600f5736-35ec-476...|50b165d0-6486-4d5...|2023-03-26|     P1097| 94.63|\n",
      "|f35f20fb-bc93-417...|4657a2b1-abae-49a...|2023-06-02|     P1012| 64.52|\n",
      "|98ce805f-e468-458...|4657a2b1-abae-49a...|2024-11-09|     P1077| 426.8|\n",
      "|9c664906-904a-42d...|4657a2b1-abae-49a...|2024-04-29|     P1093|407.42|\n",
      "|9ed0ed7b-2e13-4b6...|0ffe272a-f261-450...|2026-07-16|     P1048| 79.92|\n",
      "|f66d976d-888a-409...|0ffe272a-f261-450...|2023-12-22|     P1080|284.98|\n",
      "|90af474d-1e4e-430...|0ffe272a-f261-450...|2025-07-23|     P1090|297.13|\n",
      "|08c02e2d-239f-49b...|0ffe272a-f261-450...|2023-12-03|     P1029|  42.0|\n",
      "|646c0892-629b-47d...|ca9191a8-f736-46c...|2024-02-12|     P1012|430.55|\n",
      "|dcc454f1-f76d-47b...|ca9191a8-f736-46c...|2025-10-19|     P1081|153.43|\n",
      "|00da0c3f-e3ce-49a...|ca9191a8-f736-46c...|2025-09-18|     P1045| 98.07|\n",
      "|bb7cbb63-ceca-41d...|808301e6-260a-47a...|2023-07-27|     P1087|356.87|\n",
      "+--------------------+--------------------+----------+----------+------+\n",
      "only showing top 20 rows\n"
     ]
    }
   ],
   "source": [
    "# load the data\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "orders_df = spark.read.csv(\"file:///workspaces/trng-2286/datasets/orders.csv\", inferSchema=True, header=True)\n",
    "\n",
    "orders_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4a0a4445",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+----+-------+--------------------+-----------+-------------------+---------+-----------+---------+---------------+------------------+-------------+----------+---------+\n",
      "|         customer_id|               email| age| gender|             country|signup_date|         last_login|is_active|total_spent|age_group|pref_newsletter|pref_notifications|pref_language|first_name|last_name|\n",
      "+--------------------+--------------------+----+-------+--------------------+-----------+-------------------+---------+-----------+---------+---------------+------------------+-------------+----------+---------+\n",
      "|0e99a07c-c7a5-43d...|robinjackson@wrig...|50.0| Female|              France| 2023-03-01|2025-05-29 22:36:25|     true|     1438.4|    Adult|           true|              push|           en|    Thomas|     Lamb|\n",
      "|3a69ac3e-6726-431...|susan51@johnson-g...|20.0|   Male|       Guinea-Bissau| 2020-12-14|2025-03-21 23:52:55|     true|    2364.98|    Young|           true|              push|           fr|  Kimberly|    Blake|\n",
      "|c63cab5f-dc06-484...|leahwilliams@gmai...|50.0| Female|               Kenya| 2023-11-16|2024-09-05 04:59:24|    false|    5913.19|    Adult|           true|             email|           de|   William|   Taylor|\n",
      "|50b165d0-6486-4d5...|   brian56@gmail.com|27.0|  Other|              Mexico| 2021-09-03|2025-02-06 07:51:31|     true|     972.82|    Young|           true|               sms|           es|    Amanda|   Wright|\n",
      "|4657a2b1-abae-49a...|danieldiaz@hendri...|69.0|Unknown|              Taiwan| 2022-11-05|2025-06-14 04:15:42|     true|     9732.5|   Senior|          false|             email|           es|     Molly|   Watson|\n",
      "|0ffe272a-f261-450...|    lisa87@gmail.com|50.0| Female|Bouvet Island (Bo...| 2023-07-14|2025-03-05 14:30:02|     true|     505.95|    Adult|           true|               sms|           en|     Kelly|     Boyd|\n",
      "|ca9191a8-f736-46c...|    jperry@gmail.com|50.0|  Other|Saint Pierre and ...| 2023-09-02|2024-10-01 20:41:01|    false|    6375.06|    Adult|          false|             email|           es|    Jeremy|     Rios|\n",
      "|808301e6-260a-47a...|megansaunders@lee...|50.0|   Male|          Guadeloupe| 2022-01-27|2025-04-01 11:16:10|     true|    5364.69|    Adult|           true|             email|           de|     Jason|    Hicks|\n",
      "|997d2ea4-5957-43d...|allenrachel@hotma...|58.0|  Other|           Sri Lanka| 2024-10-20|2025-02-10 23:58:09|     true|    2329.03|    Adult|           true|               sms|           de|    Rickey|   Snyder|\n",
      "|f1d943c5-9fba-4c9...|   james40@gmail.com|50.0| Female|               Congo| 2024-06-18|2025-01-14 12:13:09|    false|    3986.54|    Adult|          false|             email|           es|  Stefanie|    Moore|\n",
      "|438f6971-c3b4-4fe...|coxkristen@torres...|50.0|Unknown|    Papua New Guinea| 2020-07-24|2025-04-12 03:00:13|    false|    3651.86|    Adult|           true|              push|           de|     David|   Wilson|\n",
      "|3457b0f6-1777-449...|   mark17@smith.info|50.0|  Other|Saint Vincent and...| 2022-03-04|2024-09-28 20:31:11|     true|    6818.82|    Adult|           true|              push|           es|   Kristin|   Barker|\n",
      "|adbc75ff-9d33-446...|vanceamanda@yahoo...|39.0|  Other|           Guatemala| 2023-09-16|2025-03-04 14:43:28|    false|    1623.67|    Adult|           true|              push|           es|   Anthony|     Hart|\n",
      "|a97da932-82a3-4ec...|youngjacob@yahoo.com|66.0|   Male|           Mauritius| 2025-06-24|2024-08-02 21:59:11|    false|    8424.61|   Senior|           true|             email|           es|    Carrie|   Hughes|\n",
      "|a5767eda-bd1d-482...|goodmanlaura@hotm...|52.0|  Other|                Guam| 2023-12-26|2024-12-25 13:57:45|    false|     243.79|    Adult|          false|               sms|           fr|   Shelley|   Morrow|\n",
      "|cc7f129e-fb05-49d...| tranroy@hotmail.com|50.0|Unknown|             Tokelau| 2021-08-24|2025-01-13 13:17:33|     true|    9292.69|    Adult|           true|              push|           es|    Andrew|  Stanley|\n",
      "|9977d5e4-6b2d-418...|rosalesgeorge@mar...|50.0|Unknown|      United Kingdom| 2022-07-20|2024-08-21 03:30:28|    false|    1253.99|    Adult|           true|             email|           es|     Tyler| Crawford|\n",
      "|9b330788-2219-400...|     sgay@barnes.com|39.0| Female|Saint Vincent and...| 2023-01-15|2024-09-12 18:29:24|    false|    6706.33|    Adult|           true|              push|           fr|      Ryan|   Gibson|\n",
      "|c0d08fef-2f97-410...|christine76@gmail...|50.0| Female|          Bangladesh| 2024-09-27|2025-06-25 09:07:54|    false|     8084.6|    Adult|           true|             email|           en|     Jacob|   Dodson|\n",
      "|1c462dc3-44aa-413...|zacharysilva@jord...|50.0|Unknown|    French Polynesia| 2021-02-09|2025-04-26 10:00:34|    false|    8645.18|    Adult|           true|              push|           en|      Lisa|   Pierce|\n",
      "+--------------------+--------------------+----+-------+--------------------+-----------+-------------------+---------+-----------+---------+---------------+------------------+-------------+----------+---------+\n",
      "only showing top 20 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "customers_df = spark.read.parquet(\"file:///workspaces/trng-2286/datasets/final_customer_data.parquet\")\n",
    "\n",
    "customers_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "28744049",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# partitionng in memeory - repartition\n",
    "\n",
    "customers_df.repartition(4, \"country\").write.mode(\"overwrite\").parquet(\"./datasets/customers_partitioned\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51dce1fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "partitioned_customer_df = customers_df.repartitionByRange(4, \"country\").sortWithinPartitions(\"total_spent\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "064cba39",
   "metadata": {},
   "source": [
    "| Method                       | Function                 | Shuffles?               | Partitions  | Use Case                        |\n",
    "| ---------------------------- | ------------------------ | ----------------------- | ----------- | ------------------------------- |\n",
    "| `repartition(n, col)`        | Hash repartition         | Yes (full shuffle)      | Exact count | General repartitioning          |\n",
    "| `repartitionByRange(n, col)` | Range-based partitioning | Yes (efficient shuffle) | Exact count | Sorted / range-based processing |\n",
    "| `sortWithinPartitions(col)`  | Local sort               | No shuffle              | As is       | Ordered rows per partition      |\n",
    "| `spark_partition_id()`       | Partition tracking       | No                      | -           | Debugging/analysis              |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6740d32a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+---------------------------------+-----------+\n",
      "|partition_id|country                          |total_spent|\n",
      "+------------+---------------------------------+-----------+\n",
      "|0           |Bouvet Island (Bouvetoya)        |505.95     |\n",
      "|0           |Cocos (Keeling) Islands          |509.83     |\n",
      "|0           |Belarus                          |901.85     |\n",
      "|0           |Azerbaijan                       |1800.07    |\n",
      "|0           |Brunei Darussalam                |2351.05    |\n",
      "|0           |Argentina                        |2475.12    |\n",
      "|0           |Cuba                             |2707.21    |\n",
      "|0           |Bhutan                           |2888.23    |\n",
      "|0           |Cambodia                         |2997.65    |\n",
      "|0           |Bangladesh                       |3189.7     |\n",
      "|0           |Central African Republic         |3580.51    |\n",
      "|0           |Congo                            |3986.54    |\n",
      "|0           |Colombia                         |4283.76    |\n",
      "|0           |Anguilla                         |4445.77    |\n",
      "|0           |Albania                          |4882.13    |\n",
      "|0           |Bermuda                          |5178.75    |\n",
      "|0           |Brunei Darussalam                |5845.97    |\n",
      "|0           |Anguilla                         |6068.06    |\n",
      "|0           |Chile                            |6273.87    |\n",
      "|0           |Bolivia                          |6692.4     |\n",
      "|0           |Bangladesh                       |8084.6     |\n",
      "|0           |Cape Verde                       |8120.86    |\n",
      "|0           |Belize                           |8302.01    |\n",
      "|1           |Guam                             |243.79     |\n",
      "|1           |Greenland                        |548.92     |\n",
      "|1           |Faroe Islands                    |975.88     |\n",
      "|1           |France                           |1438.4     |\n",
      "|1           |Guatemala                        |1623.67    |\n",
      "|1           |Guinea-Bissau                    |2364.98    |\n",
      "|1           |Finland                          |2560.1     |\n",
      "|1           |Iran                             |2885.62    |\n",
      "|1           |El Salvador                      |3086.65    |\n",
      "|1           |Cyprus                           |3244.53    |\n",
      "|1           |France                           |4026.55    |\n",
      "|1           |Jamaica                          |4383.97    |\n",
      "|1           |Guinea                           |4493.15    |\n",
      "|1           |Egypt                            |4713.54    |\n",
      "|1           |Guadeloupe                       |5364.69    |\n",
      "|1           |Eritrea                          |6174.37    |\n",
      "|1           |Guinea                           |7112.34    |\n",
      "|1           |Gambia                           |7517.24    |\n",
      "|1           |Heard Island and McDonald Islands|7823.27    |\n",
      "|1           |French Polynesia                 |8645.18    |\n",
      "|1           |Hungary                          |8702.54    |\n",
      "|1           |Jamaica                          |9670.55    |\n",
      "|2           |Mexico                           |972.82     |\n",
      "|2           |Madagascar                       |1855.82    |\n",
      "|2           |Saint Barthelemy                 |2765.71    |\n",
      "|2           |Papua New Guinea                 |3651.86    |\n",
      "|2           |Poland                           |3674.0     |\n",
      "+------------+---------------------------------+-----------+\n",
      "only showing top 50 rows\n"
     ]
    }
   ],
   "source": [
    "partitioned_customer_df.withColumn(\"partition_id\", spark_partition_id()) \\\n",
    ".select(\"partition_id\", \"country\", \"total_spent\") \\\n",
    ".orderBy(\"partition_id\", \"total_spent\") \\\n",
    ".show(50, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1b6e3be1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# partiion in memory - coalesce \n",
    "\n",
    "customers_df.coalesce(1).write.mode(\"overwrite\").parquet(\"./datasets/customers_coalesce\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5a21b644",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# partition by disk\n",
    "\n",
    "customers_df.write.mode(\"overwrite\").partitionBy(\"country\").parquet(\"./datasets/customers_by_country\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2e0fe78b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Parsed Logical Plan ==\n",
      "'Filter '`=`('country, Finland)\n",
      "+- Relation [customer_id#131,email#132,age#133,gender#134,signup_date#135,last_login#136,is_active#137,total_spent#138,age_group#139,pref_newsletter#140,pref_notifications#141,pref_language#142,first_name#143,last_name#144,country#145] parquet\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "customer_id: string, email: string, age: double, gender: string, signup_date: date, last_login: timestamp, is_active: boolean, total_spent: double, age_group: string, pref_newsletter: boolean, pref_notifications: string, pref_language: string, first_name: string, last_name: string, country: string\n",
      "Filter (country#145 = Finland)\n",
      "+- Relation [customer_id#131,email#132,age#133,gender#134,signup_date#135,last_login#136,is_active#137,total_spent#138,age_group#139,pref_newsletter#140,pref_notifications#141,pref_language#142,first_name#143,last_name#144,country#145] parquet\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Filter (isnotnull(country#145) AND (country#145 = Finland))\n",
      "+- Relation [customer_id#131,email#132,age#133,gender#134,signup_date#135,last_login#136,is_active#137,total_spent#138,age_group#139,pref_newsletter#140,pref_notifications#141,pref_language#142,first_name#143,last_name#144,country#145] parquet\n",
      "\n",
      "== Physical Plan ==\n",
      "*(1) ColumnarToRow\n",
      "+- FileScan parquet [customer_id#131,email#132,age#133,gender#134,signup_date#135,last_login#136,is_active#137,total_spent#138,age_group#139,pref_newsletter#140,pref_notifications#141,pref_language#142,first_name#143,last_name#144,country#145] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/workspaces/trng-2286/datasets/customers_by_country], PartitionFilters: [isnotnull(country#145), (country#145 = Finland)], PushedFilters: [], ReadSchema: struct<customer_id:string,email:string,age:double,gender:string,signup_date:date,last_login:times...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_partitioned = spark.read.parquet(\"file:///workspaces/trng-2286/datasets/customers_by_country\")\n",
    "\n",
    "df_partitioned.filter(col(\"country\") == \"Finland\").explain(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ef31d1c",
   "metadata": {},
   "source": [
    "\n",
    "### Bucketing\n",
    "\n",
    "- Bucketing organizes data into fixed number of buckets using the hash of a column.\n",
    "\n",
    "**Benefits:**\n",
    "- Reduces shuffle during joins and aggregations.\n",
    "- Supports efficient bucketed joins and sort-merge joins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90910c44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bucketing\n",
    "spark.sql(\"DROP TABLE IF EXISTS bucketed_customers\")\n",
    "\n",
    "customers_df.write.bucketBy(8, \"customer_id\") \\\n",
    "        .sortBy(\"age\") \\\n",
    "        .mode(\"overwrite\") \\\n",
    "        .saveAsTable(\"bucketed_customers\")\n",
    "\n",
    "\n",
    "spark.sql(\"DROP TABLE IF EXISTS bucketed_orders\")\n",
    "\n",
    "orders_df.write.bucketBy(8, \"customer_id\") \\\n",
    "        .sortBy(\"order_date\") \\\n",
    "        .mode(\"overwrite\") \\\n",
    "        .saveAsTable(\"bucketed_orders\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "83d26e6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Parsed Logical Plan ==\n",
      "'Join UsingJoin(Inner, [customer_id])\n",
      ":- SubqueryAlias spark_catalog.default.bucketed_orders\n",
      ":  +- Relation spark_catalog.default.bucketed_orders[order_id#167,customer_id#168,order_date#169,product_id#170,amount#171] parquet\n",
      "+- SubqueryAlias spark_catalog.default.bucketed_customers\n",
      "   +- Relation spark_catalog.default.bucketed_customers[customer_id#152,email#153,age#154,gender#155,country#156,signup_date#157,last_login#158,is_active#159,total_spent#160,age_group#161,pref_newsletter#162,pref_notifications#163,pref_language#164,first_name#165,last_name#166] parquet\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "customer_id: string, order_id: string, order_date: date, product_id: string, amount: double, email: string, age: double, gender: string, country: string, signup_date: date, last_login: timestamp, is_active: boolean, total_spent: double, age_group: string, pref_newsletter: boolean, pref_notifications: string, pref_language: string, first_name: string, last_name: string\n",
      "Project [customer_id#168, order_id#167, order_date#169, product_id#170, amount#171, email#153, age#154, gender#155, country#156, signup_date#157, last_login#158, is_active#159, total_spent#160, age_group#161, pref_newsletter#162, pref_notifications#163, pref_language#164, first_name#165, last_name#166]\n",
      "+- Join Inner, (customer_id#168 = customer_id#152)\n",
      "   :- SubqueryAlias spark_catalog.default.bucketed_orders\n",
      "   :  +- Relation spark_catalog.default.bucketed_orders[order_id#167,customer_id#168,order_date#169,product_id#170,amount#171] parquet\n",
      "   +- SubqueryAlias spark_catalog.default.bucketed_customers\n",
      "      +- Relation spark_catalog.default.bucketed_customers[customer_id#152,email#153,age#154,gender#155,country#156,signup_date#157,last_login#158,is_active#159,total_spent#160,age_group#161,pref_newsletter#162,pref_notifications#163,pref_language#164,first_name#165,last_name#166] parquet\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Project [customer_id#168, order_id#167, order_date#169, product_id#170, amount#171, email#153, age#154, gender#155, country#156, signup_date#157, last_login#158, is_active#159, total_spent#160, age_group#161, pref_newsletter#162, pref_notifications#163, pref_language#164, first_name#165, last_name#166]\n",
      "+- Join Inner, (customer_id#168 = customer_id#152)\n",
      "   :- Filter isnotnull(customer_id#168)\n",
      "   :  +- Relation spark_catalog.default.bucketed_orders[order_id#167,customer_id#168,order_date#169,product_id#170,amount#171] parquet\n",
      "   +- Filter isnotnull(customer_id#152)\n",
      "      +- Relation spark_catalog.default.bucketed_customers[customer_id#152,email#153,age#154,gender#155,country#156,signup_date#157,last_login#158,is_active#159,total_spent#160,age_group#161,pref_newsletter#162,pref_notifications#163,pref_language#164,first_name#165,last_name#166] parquet\n",
      "\n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- Project [customer_id#168, order_id#167, order_date#169, product_id#170, amount#171, email#153, age#154, gender#155, country#156, signup_date#157, last_login#158, is_active#159, total_spent#160, age_group#161, pref_newsletter#162, pref_notifications#163, pref_language#164, first_name#165, last_name#166]\n",
      "   +- BroadcastHashJoin [customer_id#168], [customer_id#152], Inner, BuildLeft, false\n",
      "      :- BroadcastExchange HashedRelationBroadcastMode(List(input[1, string, false]),false), [plan_id=444]\n",
      "      :  +- Filter isnotnull(customer_id#168)\n",
      "      :     +- FileScan parquet spark_catalog.default.bucketed_orders[order_id#167,customer_id#168,order_date#169,product_id#170,amount#171] Batched: true, Bucketed: false (disabled by query planner), DataFilters: [isnotnull(customer_id#168)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/workspaces/trng-2286/spark-warehouse/bucketed_orders], PartitionFilters: [], PushedFilters: [IsNotNull(customer_id)], ReadSchema: struct<order_id:string,customer_id:string,order_date:date,product_id:string,amount:double>\n",
      "      +- Filter isnotnull(customer_id#152)\n",
      "         +- FileScan parquet spark_catalog.default.bucketed_customers[customer_id#152,email#153,age#154,gender#155,country#156,signup_date#157,last_login#158,is_active#159,total_spent#160,age_group#161,pref_newsletter#162,pref_notifications#163,pref_language#164,first_name#165,last_name#166] Batched: true, Bucketed: false (disabled by query planner), DataFilters: [isnotnull(customer_id#152)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/workspaces/trng-2286/spark-warehouse/bucketed_customers], PartitionFilters: [], PushedFilters: [IsNotNull(customer_id)], ReadSchema: struct<customer_id:string,email:string,age:double,gender:string,country:string,signup_date:date,l...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "bucketed_customers = spark.table(\"bucketed_customers\")\n",
    "bucketed_orders = spark.table(\"bucketed_orders\")\n",
    "\n",
    "bucketed_orders.join(bucketed_customers, \"customer_id\").explain(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fe977b7",
   "metadata": {},
   "source": [
    "| Feature               | **Partitioning**                                                 | **Bucketing**                                                            |\n",
    "| --------------------- | ---------------------------------------------------------------- | ------------------------------------------------------------------------ |\n",
    "| **Definition**        | Divides data into **directory-based partitions** by column value | Divides data into a **fixed number of buckets** using a hash of a column |\n",
    "| **Granularity**       | Coarse-grained (1 partition per unique value)                    | Fine-grained (fixed number of buckets, regardless of unique values)      |\n",
    "| **Data Layout**       | Creates folders for each partition column value                  | Creates files (buckets) within a single folder                           |\n",
    "| **Shuffle Required?** | Yes (during writing and often during reading)                    | Yes (during writing, but optimized join/scan during reading)             |\n",
    "| **Use Case**          | Filter pushdown and pruning (`WHERE country = 'IN'`)             | Efficient **joins** and **sampling** on large datasets                   |\n",
    "| **Syntax (Write)**    | `.write.partitionBy(\"col\")`                                      | `.write.bucketBy(4, \"col\").sortBy(\"col\").saveAsTable(...)`               |\n",
    "| **Sort Support**      | Not sorted by default                                            | Can be sorted within each bucket (`.sortBy(...)`)                        |\n",
    "| **Join Optimization** | Not directly useful                                              | Enables **bucketed joins** (avoids shuffle if both sides are bucketed)   |\n",
    "| **Flexibility**       | Dynamically adjusts to data                                      | Requires fixed bucket count defined in advance                           |\n",
    "| **Storage Format**    | Works with any format (Parquet, Delta, etc.)                     | Only supported when using `.saveAsTable()` (Hive-compatible)             |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c29412c",
   "metadata": {},
   "source": [
    "### Joins\n",
    "\n",
    "In PySpark, joins combine rows from two DataFrames based on a common key (just like SQL joins).\n",
    "\n",
    "| Join Type | Description                                        |\n",
    "| --------- | -------------------------------------------------- |\n",
    "| `inner`   | Keep only matching rows in both DataFrames         |\n",
    "| `left`    | All rows from left + matching from right           |\n",
    "| `right`   | All rows from right + matching from left           |\n",
    "| `outer`   | All rows from both (NULL where no match)           |\n",
    "| `semi`    | Keep rows from left **if match exists** in right   |\n",
    "| `anti`    | Keep rows from left **if no match** in right       |\n",
    "| `cross`   | Cartesian product (every row with every other row) |\n",
    "\n",
    "\n",
    "\n",
    "### Broadcast Joins\n",
    "\n",
    "A broadcast join is an optimization technique in Spark that sends a small DataFrame to all worker nodes, so the larger DataFrame doesn’t need to shuffle its data.\n",
    "\n",
    "useful when:\n",
    "\n",
    "- Joins are expensive because they involve data shuffling across nodes\n",
    "- Broadcast joins eliminate shuffle if one DataFrame is small enough to fit in memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5dbda102",
   "metadata": {},
   "outputs": [],
   "source": [
    "# large df\n",
    "employees_df = spark.createDataFrame([\n",
    "    (1, \"Alice\", \"HR\"),\n",
    "    (2, \"Bob\", \"IT\"),\n",
    "    (3, \"Charlie\", \"IT\"),\n",
    "    (4, \"David\", \"Finance\"),\n",
    "    (5, \"Eve\", \"HR\")\n",
    "], [\"emp_id\", \"name\", \"dept\"])\n",
    "\n",
    "# small df \n",
    "\n",
    "depratments_df = spark.createDataFrame([\n",
    "    (\"Finance\", \"Finance & Accounts\"),\n",
    "    (\"IT\", \"Information Technology\"),\n",
    "    (\"HR\", \"Human Resources\")\n",
    "], [\"dept\", \"dept_desc\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "33c96b71",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 25:>                                                         (0 + 2) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+-------+--------------------+\n",
      "|   dept|emp_id|   name|           dept_desc|\n",
      "+-------+------+-------+--------------------+\n",
      "|Finance|     4|  David|  Finance & Accounts|\n",
      "|     HR|     1|  Alice|     Human Resources|\n",
      "|     HR|     5|    Eve|     Human Resources|\n",
      "|     IT|     2|    Bob|Information Techn...|\n",
      "|     IT|     3|Charlie|Information Techn...|\n",
      "+-------+------+-------+--------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# default join on department\n",
    "\n",
    "joined_df = employees_df.join(depratments_df, on=\"dept\", how=\"inner\")\n",
    "\n",
    "joined_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "16ba6303",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Parsed Logical Plan ==\n",
      "'Join UsingJoin(Inner, [dept])\n",
      ":- LogicalRDD [emp_id#184L, name#185, dept#186], false\n",
      "+- ResolvedHint (strategy=broadcast)\n",
      "   +- LogicalRDD [dept#187, dept_desc#188], false\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "dept: string, emp_id: bigint, name: string, dept_desc: string\n",
      "Project [dept#186, emp_id#184L, name#185, dept_desc#188]\n",
      "+- Join Inner, (dept#186 = dept#187)\n",
      "   :- LogicalRDD [emp_id#184L, name#185, dept#186], false\n",
      "   +- ResolvedHint (strategy=broadcast)\n",
      "      +- LogicalRDD [dept#187, dept_desc#188], false\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Project [dept#186, emp_id#184L, name#185, dept_desc#188]\n",
      "+- Join Inner, (dept#186 = dept#187), rightHint=(strategy=broadcast)\n",
      "   :- Filter isnotnull(dept#186)\n",
      "   :  +- LogicalRDD [emp_id#184L, name#185, dept#186], false\n",
      "   +- Filter isnotnull(dept#187)\n",
      "      +- LogicalRDD [dept#187, dept_desc#188], false\n",
      "\n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- Project [dept#186, emp_id#184L, name#185, dept_desc#188]\n",
      "   +- BroadcastHashJoin [dept#186], [dept#187], Inner, BuildRight, false\n",
      "      :- Filter isnotnull(dept#186)\n",
      "      :  +- Scan ExistingRDD[emp_id#184L,name#185,dept#186]\n",
      "      +- BroadcastExchange HashedRelationBroadcastMode(List(input[0, string, false]),false), [plan_id=586]\n",
      "         +- Filter isnotnull(dept#187)\n",
      "            +- Scan ExistingRDD[dept#187,dept_desc#188]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# broadcast join on small df\n",
    "\n",
    "broadcast_joined_df = employees_df.join(broadcast(depratments_df), on=\"dept\", how=\"inner\")\n",
    "\n",
    "broadcast_joined_df.explain(True)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
