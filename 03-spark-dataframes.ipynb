{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c57eff29",
   "metadata": {},
   "source": [
    "# Spark DataFrames API\n",
    "\n",
    "\n",
    "### Dataframe\n",
    "\n",
    "- DataFrames are ditributed collections of records, all with pre-defined structure(schema - structure and data types of all columns)\n",
    "-  DataFrames are built on Spark's core concepts but with structure, optimization and SQL-like operations for data manipulation.\n",
    "- DataFrames track their schema and provide native support for many common SQL functions and relational operators\n",
    "- DataFrames are evaluated as DAGs, using lazy evaluation and providing lineage and fault tolerance.\n",
    "- DataFrames are immutable\n",
    "\n",
    "### SparkContext vs SparkSession\n",
    "\n",
    "- SparkSession is Spark application entry point. \n",
    "- Introduced in spark 2.0 as a unified entry point for all contexts (formerly instantiated individually as SparkContext, SQLContext, HiveContext, StreamingContext)\n",
    "\n",
    "<i>Note: In databricks it is automatically created for you as spark</i>\n",
    "\n",
    "### DataFrame API Optimizations\n",
    "\n",
    "- **Adaptive Query Execution:** Dynamic plan adjustments during runtime based on actual data characteristics and execution patterns.\n",
    "- **In-Memory Columnar Storage(Tungsten):** In-Memory coloumnar format for all the DataFrames enabling efficient analytical query performance and reduced memory footprint.\n",
    "- **Built-in Statistics** - Automatic statistics collection when saving to optimized formats (Parqurt, Delta in databricks) enables smarter query planning and execution.\n",
    "- **Catalyst Optimizer:** Query optimization engine that coverts DataFrame operations into an optimized execution plan\n",
    "\n",
    "\n",
    "<i>**Note** Databricks comes with a native vectorized query engine that accelerates query execution using photon engine</i>\n",
    "\n",
    "**DataFrame Query Planning:** \n",
    "\n",
    "- When a DataFrame is evaluated, the driver creates an optimized execution plan through a series of transformations \n",
    "- Converts the logical plan into phycal execution that minimizes resource usage and execution time. (Unresolved LP -> analysed LP -> optimized LP -> Physical Plan)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cf09c99",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9096a9b8",
   "metadata": {},
   "source": [
    "\n",
    "### DataFrame Data Types\n",
    "\n",
    "#### Primitive\n",
    "\n",
    "**`pyspark.sql.types.DataType`**\n",
    "\n",
    "- `ByteType`\n",
    "- `ShortType`\n",
    "- `IntegerType`\n",
    "- `LongType`\n",
    "- `FloatType`\n",
    "- `DoubleType`\n",
    "- `BooleanType`\n",
    "- `StringType`\n",
    "- `BinaryType`\n",
    "- `TimestampType`\n",
    "- `DateType`\n",
    "\n",
    "#### complex data types\n",
    "\n",
    "- `ArrayType`\n",
    "- `MapType`\n",
    "- `StructType`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c9688e9",
   "metadata": {},
   "source": [
    "### Common DataFrame API methods\n",
    "\n",
    "#### Transformations\n",
    "\n",
    "##### Narrow Transformations\n",
    "\n",
    "- narrow transformations process data within each partition independetly, without needing to combine data from other partitions.\n",
    "- faster and more efficient because they avoid data shuffling between partitions. \n",
    "\n",
    "1. `select()` : selecting specific rows\n",
    "2. `filter()`: Applying a filter condition to rows. \n",
    "3. `map()`: Applying a function to each row. \n",
    "4. `union()`: Combining two DataFrames with identical schemas. \n",
    "5. `withColumn()`: Adding a new column based on existing ones. \n",
    "6. `drop()`: Removing a column. \n",
    "\n",
    "##### Wide Transformations\n",
    "\n",
    "- Wide transformations require data to be redistributed across partitions, often involving shuffling data based on keys.\n",
    "\n",
    "1. `groupBy()`: Grouping data based on a column, which often requires shuffling to aggregate data from different partitions. \n",
    "2. `join()`: Joining two DataFrames, which requires shuffling data to combine rows based on a join key. \n",
    "3. `distinct()`: Removing duplicate rows, which might require shuffling to compare rows across partitions. \n",
    "\n",
    "#### Actions\n",
    "\n",
    "1. `count()`: returns number of rows in a Dataframe\n",
    "2. `show()`: display DataFrame content\n",
    "3. `take(n)`: return first n rows from a DataFrame\n",
    "4. `first()`: return first row from a DataFrame\n",
    "5. `write()`: save DataFrame to storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e834e0f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "54118275",
   "metadata": {},
   "source": [
    "%md\n",
    "#### Handiling missing values\n",
    "\n",
    "**common functions**\n",
    "\n",
    "- `isNull()`/`isNotNull()` - checks if values are null\n",
    "- `count(col)` - counts non null values in a specific column\n",
    "- `df.fillna()`/`df.na.fill()` - replace nulls with values\n",
    "- `df.dropna()`/`df.na.drop()` - remove rows with nulls\n",
    "\n",
    "#### referencing columns\n",
    "\n",
    "- direct - `df.select(\"col_name\")` - basic column selection\n",
    "\n",
    "- by attribute - `df.select(df.attribute)` - column names that are valid python idenfiers, can be referenced across DataFrames(e.g., join)\n",
    "\n",
    "- column expression - `df.select(df[\"col_name\"])` - any column names, can be referenced across DataFrames\n",
    "\n",
    "- column object - `df.select(col(\"name\"))` - required when building complex expressions or using column specific operations like `cast()`, `alias()`, `asc()`or `desc()`\n",
    "\n",
    "\n",
    "#### common column object methods\n",
    "\n",
    "- `alias()` - rename column\n",
    "- `cast()` - chnage data type\n",
    "- `isNull()` or `isNotNull()` - check for nulls\n",
    "- `contains()` - string matching\n",
    "- `asc()`/`desc()` - sort direction - `df.sort(col(\"c1\").asc())`\n",
    "\n",
    "\n",
    "[pyspark.sql.Column](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.Column.html)\n",
    "\n",
    "#### common built-in functions\n",
    "\n",
    "- `concat(col1, col2)` - concatenate strings\n",
    "- `date_format(col, fmt)` - fromat date strings\n",
    "- `round(col, scale)` - round number to scale\n",
    "- `regexp_replace(col, pattern, replace)` - replace using regex\n",
    "- `coalesce(col1, col2)` - first non null value\n",
    "\n",
    "\n",
    "[pyspark built in functions](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/functions.html)\n",
    "\n",
    "#### user defined functions\n",
    "\n",
    "- allows to use python functions on dataframe columns\n",
    "- helpful to create custom reusable functions\n",
    "- can impact performance as they can not be optimized by the Catalyst optimizer and has serialiation overhead.\n",
    "\n",
    "```py\n",
    "@udf(\"data_type\")\n",
    "def function_name(name):\n",
    "    return value\n",
    "\n",
    "df.select(function_name(\"name\"))\n",
    "```\n",
    "\n",
    "<i><b>note:</b> pandas udfs are efficient becuase they operate on batches of raows instead of single rows, they leverage Apache Arrow for more efficient python-JVM serialization </i>\n",
    "\n",
    "### Aggregate functions\n",
    "\n",
    "can be applied in `agg()` method after `groupBy()` operation or directly within `select()`\n",
    "\n",
    "- `sum()`\n",
    "- `avg()`\n",
    "- `min()`\n",
    "- `max()`\n",
    "- `count()`\n",
    "- `first()`\n",
    "- `last()`\n",
    "\n",
    "### Spark SQL\n",
    "\n",
    "- Spark SQL is a module in spark that allows you to run SQL queries on structured and semi-structured data.\n",
    "- It provides a SQL-like interface on top of Spark's powerful DataFrame API, enabling both SQL and programmatic access to data, all with the same optimized execution engine.\n",
    "\n",
    "- `createOrReplaceTempView()` : created a temp in-memory view that you can query with SQL.\n",
    "- `createGlobalTempView()` : Creates a global temporary view accessible across sessions (under global_temp database)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "812d7936",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "df8a2cd3",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
