{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ee4c4620",
   "metadata": {},
   "source": [
    "# AWS Kinesis – Overview and Use Cases\n",
    "\n",
    "## What is AWS Kinesis?\n",
    "\n",
    "Amazon Kinesis is a fully managed platform for real-time data ingestion, processing, and analysis at scale. It enables collection and processing of large streams of data in near real-time.\n",
    "\n",
    "A shard is the base unit of capacity in Amazon Kinesis Data Streams. It defines how much data your stream can ingest and deliver.\n",
    "\n",
    "\n",
    "**Each Shard Provides:**\n",
    "\n",
    "**Write capacity:**\n",
    "\n",
    "    - 1 MB/sec OR 1,000 records/sec\n",
    "\n",
    "**Read capacity:**\n",
    "\n",
    "    - 2 MB/sec for shared throughput\n",
    "    - Up to 2 MB/sec per consumer for enhanced fan-out\n",
    "\n",
    "## Core Components of AWS Kinesis\n",
    "\n",
    "### 1. Kinesis Data Streams (KDS)\n",
    "- Real-time, low-latency ingestion\n",
    "- Scalable by shards\n",
    "- Producers and consumers architecture\n",
    "\n",
    "**Use Cases:**\n",
    "- Clickstream tracking\n",
    "- Fraud detection\n",
    "- Application telemetry\n",
    "\n",
    "### 2. Kinesis Data Firehose\n",
    "- Serverless delivery to S3, Redshift, OpenSearch, Splunk\n",
    "- Supports Lambda transformations\n",
    "- No management needed\n",
    "\n",
    "**Use Cases:**\n",
    "- ETL pipelines\n",
    "- Log persistence\n",
    "- Analytics ingestion\n",
    "\n",
    "### 3. Kinesis Data Analytics\n",
    "- SQL and Apache Flink for real-time stream analysis\n",
    "- Windowed aggregations, filtering, joins\n",
    "- Integrates with Data Streams and Firehose\n",
    "\n",
    "**Use Cases:**\n",
    "- Real-time analytics\n",
    "- Stream enrichment\n",
    "- Trend monitoring\n",
    "\n",
    "### 4. Kinesis Video Streams\n",
    "- Ingests and stores video streams\n",
    "- Integrates with SageMaker and ML services\n",
    "\n",
    "**Use Cases:**\n",
    "- Surveillance and security\n",
    "- Live stream analysis\n",
    "- Video-based ML inference\n",
    "\n",
    "\n",
    "## Comparison of Components\n",
    "\n",
    "| Feature            | Data Streams | Data Firehose | Data Analytics | Video Streams |\n",
    "|--------------------|--------------|----------------|----------------|----------------|\n",
    "| Purpose            | Ingest data  | Deliver data   | Analyze stream | Analyze video  |\n",
    "| Transformations    | Custom code  | Lambda         | SQL / Flink    | Frame analysis |\n",
    "| Storage            | Temporary    | S3, Redshift   | N/A            | Built-in       |\n",
    "| Latency            | Low          | Medium         | Low            | Varies         |\n",
    "| Complexity         | Medium       | Low            | Medium         | Medium         |\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Common Use Cases\n",
    "\n",
    "| Domain             | Use Case Example                              |\n",
    "|--------------------|-----------------------------------------------|\n",
    "| Analytics          | Real-time dashboards, metrics ingestion       |\n",
    "| Security           | Intrusion detection, log processing           |\n",
    "| IoT                | Sensor data processing                        |\n",
    "| Retail             | Real-time clickstream for personalization     |\n",
    "| Media              | Live video stream processing                  |\n",
    "| Finance            | Trade analysis, fraud detection               |\n",
    "\n",
    "----\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d301bc7",
   "metadata": {},
   "source": [
    "# Spark Structured Streaming\n",
    "\n",
    "## What is Spark Structured Streaming?\n",
    "\n",
    "**Structured Streaming** is a scalable and fault-tolerant stream processing engine built on the **Spark SQL engine**. It allows developers to write streaming jobs the same way they write batch jobs using **DataFrames** and **SQL**.\n",
    "\n",
    "**Key Features:**\n",
    "- Uses **DataFrame API** and **Spark SQL** for stream processing\n",
    "- Supports **exactly-once** semantics\n",
    "- Provides **high-level abstractions** (e.g., windowing, joins, aggregations)\n",
    "- Integrates seamlessly with **batch pipelines**\n",
    "- Supports **event time** processing with **watermarking**\n",
    "\n",
    "Structured Streaming treats a streaming computation as a series of small batch computations, called **micro-batches**.\n",
    "\n",
    "\n",
    "\n",
    "### Stream Reader\n",
    "\n",
    "The **stream reader** defines the source of streaming data.\n",
    "\n",
    "**Common Sources:**\n",
    "- Kafka\n",
    "- File systems (CSV, JSON, Parquet, etc.)\n",
    "- Rate (for testing)\n",
    "- Delta tables\n",
    "\n",
    "**Example:**\n",
    "\n",
    "```python\n",
    "df = spark.readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"localhost:9092\") \\\n",
    "    .option(\"subscribe\", \"topic_name\") \\\n",
    "    .load()\n",
    "````\n",
    "\n",
    "\n",
    "### Stream Writer\n",
    "\n",
    "The **stream writer** defines how the output should be written.\n",
    "\n",
    "**Common Sink Formats:**\n",
    "\n",
    "* console\n",
    "* file (csv, json, parquet, delta)\n",
    "* memory\n",
    "* kafka\n",
    "* table\n",
    "\n",
    "**Example:**\n",
    "\n",
    "```python\n",
    "query = df.writeStream \\\n",
    "    .format(\"console\") \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .start()\n",
    "```\n",
    "\n",
    "**Output Modes:**\n",
    "\n",
    "* `append`: Only new rows\n",
    "* `update`: Updated rows only\n",
    "* `complete`: Full result table (used with aggregations)\n",
    "\n",
    "\n",
    "### Checkpointing\n",
    "\n",
    "**Checkpointing** stores metadata and intermediate state to allow:\n",
    "\n",
    "* Fault tolerance\n",
    "* Stateful processing\n",
    "* Resuming streams on failure\n",
    "\n",
    "**Usage:**\n",
    "\n",
    "```python\n",
    ".writeStream \\\n",
    "    .option(\"checkpointLocation\", \"/path/to/checkpoint\") \\\n",
    "    .start()\n",
    "```\n",
    "\n",
    "Without checkpointing, stateful operations (like aggregations, joins) will fail.\n",
    "\n",
    "\n",
    "## Realtime data processing patterns\n",
    "\n",
    "### Windowing\n",
    "\n",
    "**Windowing** enables time-based groupings for streaming data.\n",
    "\n",
    "**Types of Windows:**\n",
    "\n",
    "* **Tumbling Window**: Fixed-length, non-overlapping\n",
    "* **Sliding Window**: Overlapping windows\n",
    "* **Session Window**: Grouped by inactivity gaps\n",
    "\n",
    "**Example:**\n",
    "\n",
    "```python\n",
    "from pyspark.sql.functions import window\n",
    "\n",
    "df.groupBy(window(\"timestamp\", \"10 minutes\")) \\\n",
    "  .count()\n",
    "```\n",
    "\n",
    "\n",
    "### Watermarking\n",
    "\n",
    "**Watermarking** handles **late-arriving** data in event-time based processing.\n",
    "\n",
    "**Why It's Important:**\n",
    "\n",
    "* Allows system to clean up state\n",
    "* Defines maximum allowed delay for late events\n",
    "\n",
    "**Usage:**\n",
    "\n",
    "```python\n",
    "df.withWatermark(\"event_time\", \"10 minutes\")\n",
    "```\n",
    "\n",
    "Used together with aggregations:\n",
    "\n",
    "```python\n",
    "df.withWatermark(\"event_time\", \"10 minutes\") \\\n",
    "  .groupBy(window(\"event_time\", \"5 minutes\")) \\\n",
    "  .count()\n",
    "```\n",
    "\n",
    "\n",
    "### State Management\n",
    "\n",
    "**Stateful operations** need to track data across micro-batches.\n",
    "\n",
    "**Examples of Stateful Operations:**\n",
    "\n",
    "* Grouped aggregations\n",
    "* Stream-stream joins\n",
    "* Deduplication\n",
    "* Custom state logic via `mapGroupsWithState`\n",
    "\n",
    "**State Store:** Internally stores key-value state, periodically cleaned up using watermarking or timeouts.\n",
    "\n",
    "**Example:**\n",
    "\n",
    "```python\n",
    "df.groupBy(\"id\").count()  # Maintains state per ID\n",
    "```\n",
    "\n",
    "\n",
    "### Stream Processing\n",
    "- **Definition**: Processes each incoming event individually in real-time.\n",
    "- **Tools**: Apache Kafka, Apache Flink, Spark Structured Streaming, AWS Kinesis.\n",
    "- **Use Case**: Fraud detection, stock price tracking.\n",
    "\n",
    "\n",
    "### Micro-Batching\n",
    "- **Definition**: Buffers data for a short duration (e.g., 1–2 seconds) and processes in mini-batches.\n",
    "- **Tools**: Spark Structured Streaming (uses micro-batching internally).\n",
    "- **Use Case**: Real-time log analysis with slight delay tolerance.\n",
    "\n",
    "\n",
    "### Event-Driven Pattern (Lambda Architecture)\n",
    "- **Definition**: Combines batch and stream processing.\n",
    "  - **Speed Layer**: For real-time data.\n",
    "  - **Batch Layer**: For historical data.\n",
    "  - **Serving Layer**: Combines both for querying.\n",
    "- **Use Case**: Real-time + historical analytics dashboards.\n",
    "\n",
    "\n",
    "### Event Sourcing\n",
    "- **Definition**: Records all changes as a sequence of immutable events instead of just the final state.\n",
    "- **Use Case**: Financial systems with full audit trails, systems needing replay capability.\n",
    "\n",
    "### Change Data Capture (CDC)\n",
    "- **Definition**: Captures and streams changes in a database (insert, update, delete).\n",
    "- **Tools**: Debezium, AWS DMS, Oracle GoldenGate.\n",
    "- **Use Case**: Syncing OLTP DB to real-time warehouse.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Summary Table\n",
    "\n",
    "| Concept          | Description                                   | Use Case                           |\n",
    "| ---------------- | --------------------------------------------- | ---------------------------------- |\n",
    "| Stream Reader    | Reads real-time data from source              | Kafka, files, rate source          |\n",
    "| Stream Writer    | Writes results to sink                        | Console, file, delta, memory       |\n",
    "| Checkpointing    | Stores state and progress                     | Required for stateful ops          |\n",
    "| Windowing        | Group data by event time intervals            | Time-based analytics               |\n",
    "| Watermarking     | Handles late data and controls state cleanup  | Event-time aggregations            |\n",
    "| State Management | Maintains intermediate results across batches | Aggregations, joins, deduplication |\n",
    "\n",
    "\n",
    "## Best Practices for Structured Streaming\n",
    "\n",
    "1. **Always Set a Checkpoint Location**\n",
    "\n",
    "   * Required for fault-tolerance and stateful operations.\n",
    "\n",
    "2. **Use Watermarking with Event Time**\n",
    "\n",
    "   * To prevent unbounded state growth and handle late data properly.\n",
    "\n",
    "3. **Optimize Trigger Intervals**\n",
    "\n",
    "   * Use `trigger(processingTime=\"10 seconds\")` to balance latency and throughput.\n",
    "\n",
    "4. **Choose Output Mode Carefully**\n",
    "\n",
    "   * Use `append` for insert-only data.\n",
    "   * Use `update` or `complete` when dealing with aggregations.\n",
    "\n",
    "5. **Use Efficient File Formats**\n",
    "\n",
    "   * Prefer `parquet` or `delta` over `csv/json` for performance.\n",
    "\n",
    "6. **Avoid Overloading Console Sink**\n",
    "\n",
    "   * Console sink is for debugging only, not production.\n",
    "\n",
    "7. **Monitor Streaming Jobs**\n",
    "\n",
    "    * Use Spark UI and logs to track progress, throughput, and state size.\n",
    "\n",
    "\n",
    "```\n",
    "[IoT Devices / App Logs / APIs]\n",
    "        ↓\n",
    "   [Kafka / Kinesis]\n",
    "        ↓\n",
    "[Stream Processor (Flink / Spark / Beam)]\n",
    "        ↓\n",
    "[Realtime DB (Redis / Elastic) + Datalake (S3)]\n",
    "        ↓\n",
    "[Dashboards / Alerts / ML Models]\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10abe0a0",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "#### kinesis-connector for spark streaming\n",
    "\n",
    "```sh\n",
    "sudo wget https://awslabs-code-us-east-1.s3.amazonaws.com/spark-sql-kinesis-connector/spark-streaming-sql-kinesis-connector_2.12-1.2.1.jar\n",
    "sudo chmod 755 spark-streaming-sql-kinesis-connector_2.12-1.2.1.jar\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "12644d95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyspark==3.5.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (3.5.0)\n",
      "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from pyspark==3.5.0) (0.10.9.7)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! pip install pyspark==3.5.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "16c452b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/08/08 04:26:07 WARN Utils: Your hostname, codespaces-c6070e resolves to a loopback address: 127.0.0.1; using 10.0.0.88 instead (on interface eth0)\n",
      "25/08/08 04:26:07 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "25/08/08 04:26:07 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"KinesisConsumer\") \\\n",
    "            .config(\"spark.jars\", \"/workspaces/trng-2286/spark-streaming-sql-kinesis-connector_2.12-1.2.1.jar\")\\\n",
    "            .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c84b7788",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: python-dotenv in /usr/local/python/3.12.1/lib/python3.12/site-packages (1.1.1)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! pip install python-dotenv boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aa0a3de4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "326a9b7d",
   "metadata": {},
   "source": [
    "[aws kinesis docs](https://aws.amazon.com/blogs/big-data/build-spark-structured-streaming-applications-with-the-open-source-connector-for-amazon-kinesis-data-streams/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ce60a3d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "kinesis = spark.readStream.format(\"aws-kinesis\") \\\n",
    "    .option(\"kinesis.region\", \"us-east-1\") \\\n",
    "    .option(\"kinesis.streamName\", \"demo-spark-streaming-events\") \\\n",
    "    .option(\"kinesis.consumerType\", \"GetRecords\") \\\n",
    "    .option(\"kinesis.endpointUrl\", \"https://kinesis.us-east-1.amazonaws.com\") \\\n",
    "    .option(\"kinesis.startingposition\", \"LATEST\") \\\n",
    "    .option(\"kinesis.awsAccessKeyId\", os.getenv(\"AWS_ACCESS_KEY\")) \\\n",
    "    .option(\"kinesis.awsSecretKey\", os.getenv(\"AWS_SECRET_ACCESS_KEY\")) \\\n",
    "    .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "90478e7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"id\", IntegerType()),\n",
    "    StructField(\"event\", StringType()),\n",
    "    StructField(\"value\", DoubleType()),\n",
    "    StructField(\"event_time\", TimestampType())\n",
    "])\n",
    "\n",
    "parsed_df = kinesis \\\n",
    "    .selectExpr(\"CAST(data as STRING) as json_data\") \\\n",
    "    .select(from_json(\"json_data\", schema).alias(\"data\")) \\\n",
    "    .select(\"data.*\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f3947a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "aggrgated_df = parsed_df \\\n",
    "        .withWatermark(\"event_time\", \"15 minutes\") \\\n",
    "        .groupBy(\n",
    "            window(\"event_time\", \"10 minutes\", \"5 minutes\"), # sliding window\n",
    "            col(\"id\")\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c805fc5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.streaming.forceDeleteTempCheckpointLocation\", True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b994ddd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/08/08 04:27:04 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 4\n",
      "-------------------------------------------\n",
      "+---+--------+-----+--------------------------+\n",
      "|id |event   |value|event_time                |\n",
      "+---+--------+-----+--------------------------+\n",
      "|359|purchase|43.01|2025-08-08 04:24:48.835473|\n",
      "|953|purchase|95.12|2025-08-08 04:24:50.766439|\n",
      "|67 |click   |10.93|2025-08-08 04:24:51.981412|\n",
      "|667|view    |80.1 |2025-08-08 04:24:53.197143|\n",
      "|265|purchase|70.77|2025-08-08 04:24:54.410997|\n",
      "|363|purchase|51.15|2025-08-08 04:24:55.622517|\n",
      "|840|view    |45.38|2025-08-08 04:24:56.836915|\n",
      "|418|click   |95.0 |2025-08-08 04:24:58.052577|\n",
      "|472|purchase|33.88|2025-08-08 04:24:59.26775 |\n",
      "|702|view    |92.42|2025-08-08 04:25:00.480209|\n",
      "|988|click   |49.43|2025-08-08 04:25:01.693096|\n",
      "|269|click   |62.59|2025-08-08 04:25:02.907686|\n",
      "+---+--------+-----+--------------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:KeyboardInterrupt while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/python/3.12.1/lib/python3.12/site-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/python/3.12.1/lib/python3.12/site-packages/py4j/clientserver.py\", line 511, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/python/3.12.1/lib/python3.12/socket.py\", line 707, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      1\u001b[39m query = parsed_df.writeStream \\\n\u001b[32m      2\u001b[39m     .outputMode(\u001b[33m\"\u001b[39m\u001b[33mappend\u001b[39m\u001b[33m\"\u001b[39m) \\\n\u001b[32m      3\u001b[39m     .format(\u001b[33m\"\u001b[39m\u001b[33mconsole\u001b[39m\u001b[33m\"\u001b[39m) \\\n\u001b[32m   (...)\u001b[39m\u001b[32m      6\u001b[39m     .trigger(processingTime = \u001b[33m\"\u001b[39m\u001b[33m10 seconds\u001b[39m\u001b[33m\"\u001b[39m) \\\n\u001b[32m      7\u001b[39m     .start()\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m \u001b[43mquery\u001b[49m\u001b[43m.\u001b[49m\u001b[43mawaitTermination\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/python/3.12.1/lib/python3.12/site-packages/pyspark/sql/streaming/query.py:221\u001b[39m, in \u001b[36mStreamingQuery.awaitTermination\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    219\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._jsq.awaitTermination(\u001b[38;5;28mint\u001b[39m(timeout * \u001b[32m1000\u001b[39m))\n\u001b[32m    220\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m221\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_jsq\u001b[49m\u001b[43m.\u001b[49m\u001b[43mawaitTermination\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/python/3.12.1/lib/python3.12/site-packages/py4j/java_gateway.py:1321\u001b[39m, in \u001b[36mJavaMember.__call__\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m   1314\u001b[39m args_command, temp_args = \u001b[38;5;28mself\u001b[39m._build_args(*args)\n\u001b[32m   1316\u001b[39m command = proto.CALL_COMMAND_NAME +\\\n\u001b[32m   1317\u001b[39m     \u001b[38;5;28mself\u001b[39m.command_header +\\\n\u001b[32m   1318\u001b[39m     args_command +\\\n\u001b[32m   1319\u001b[39m     proto.END_COMMAND_PART\n\u001b[32m-> \u001b[39m\u001b[32m1321\u001b[39m answer = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgateway_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1322\u001b[39m return_value = get_return_value(\n\u001b[32m   1323\u001b[39m     answer, \u001b[38;5;28mself\u001b[39m.gateway_client, \u001b[38;5;28mself\u001b[39m.target_id, \u001b[38;5;28mself\u001b[39m.name)\n\u001b[32m   1325\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/python/3.12.1/lib/python3.12/site-packages/py4j/java_gateway.py:1038\u001b[39m, in \u001b[36mGatewayClient.send_command\u001b[39m\u001b[34m(self, command, retry, binary)\u001b[39m\n\u001b[32m   1036\u001b[39m connection = \u001b[38;5;28mself\u001b[39m._get_connection()\n\u001b[32m   1037\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1038\u001b[39m     response = \u001b[43mconnection\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1039\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m binary:\n\u001b[32m   1040\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m response, \u001b[38;5;28mself\u001b[39m._create_connection_guard(connection)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/python/3.12.1/lib/python3.12/site-packages/py4j/clientserver.py:511\u001b[39m, in \u001b[36mClientServerConnection.send_command\u001b[39m\u001b[34m(self, command)\u001b[39m\n\u001b[32m    509\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    510\u001b[39m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m511\u001b[39m         answer = smart_decode(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m[:-\u001b[32m1\u001b[39m])\n\u001b[32m    512\u001b[39m         logger.debug(\u001b[33m\"\u001b[39m\u001b[33mAnswer received: \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[33m\"\u001b[39m.format(answer))\n\u001b[32m    513\u001b[39m         \u001b[38;5;66;03m# Happens when a the other end is dead. There might be an empty\u001b[39;00m\n\u001b[32m    514\u001b[39m         \u001b[38;5;66;03m# answer before the socket raises an error.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/python/3.12.1/lib/python3.12/socket.py:707\u001b[39m, in \u001b[36mSocketIO.readinto\u001b[39m\u001b[34m(self, b)\u001b[39m\n\u001b[32m    705\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m    706\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m707\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sock\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    708\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[32m    709\u001b[39m         \u001b[38;5;28mself\u001b[39m._timeout_occurred = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "query = parsed_df.writeStream \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .format(\"console\") \\\n",
    "    .option(\"truncate\", \"False\") \\\n",
    "    .option(\"checkpointLocation\", \"kinesis-checkpoint\") \\\n",
    "    .trigger(processingTime = \"10 seconds\") \\\n",
    "    .start()\n",
    "\n",
    "query.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c8c9b32",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = parsed_df.writeStream \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .format(\"csv\") \\\n",
    "    .option(\"path\", \"./datasets/events/\") \\\n",
    "    .option(\"checkpointLocation\", \"kinesis-checkpoint\") \\\n",
    "    .trigger(processingTime = \"10 seconds\") \\\n",
    "    .start()\n",
    "\n",
    "query.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e138dc2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
