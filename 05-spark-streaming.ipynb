{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ee4c4620",
   "metadata": {},
   "source": [
    "# AWS Kinesis – Overview and Use Cases\n",
    "\n",
    "## What is AWS Kinesis?\n",
    "\n",
    "Amazon Kinesis is a fully managed platform for real-time data ingestion, processing, and analysis at scale. It enables collection and processing of large streams of data in near real-time.\n",
    "\n",
    "A shard is the base unit of capacity in Amazon Kinesis Data Streams. It defines how much data your stream can ingest and deliver.\n",
    "\n",
    "\n",
    "**Each Shard Provides:**\n",
    "\n",
    "**Write capacity:**\n",
    "\n",
    "    - 1 MB/sec OR 1,000 records/sec\n",
    "\n",
    "**Read capacity:**\n",
    "\n",
    "    - 2 MB/sec for shared throughput\n",
    "    - Up to 2 MB/sec per consumer for enhanced fan-out\n",
    "\n",
    "## Core Components of AWS Kinesis\n",
    "\n",
    "### 1. Kinesis Data Streams (KDS)\n",
    "- Real-time, low-latency ingestion\n",
    "- Scalable by shards\n",
    "- Producers and consumers architecture\n",
    "\n",
    "**Use Cases:**\n",
    "- Clickstream tracking\n",
    "- Fraud detection\n",
    "- Application telemetry\n",
    "\n",
    "### 2. Kinesis Data Firehose\n",
    "- Serverless delivery to S3, Redshift, OpenSearch, Splunk\n",
    "- Supports Lambda transformations\n",
    "- No management needed\n",
    "\n",
    "**Use Cases:**\n",
    "- ETL pipelines\n",
    "- Log persistence\n",
    "- Analytics ingestion\n",
    "\n",
    "### 3. Kinesis Data Analytics\n",
    "- SQL and Apache Flink for real-time stream analysis\n",
    "- Windowed aggregations, filtering, joins\n",
    "- Integrates with Data Streams and Firehose\n",
    "\n",
    "**Use Cases:**\n",
    "- Real-time analytics\n",
    "- Stream enrichment\n",
    "- Trend monitoring\n",
    "\n",
    "### 4. Kinesis Video Streams\n",
    "- Ingests and stores video streams\n",
    "- Integrates with SageMaker and ML services\n",
    "\n",
    "**Use Cases:**\n",
    "- Surveillance and security\n",
    "- Live stream analysis\n",
    "- Video-based ML inference\n",
    "\n",
    "\n",
    "## Comparison of Components\n",
    "\n",
    "| Feature            | Data Streams | Data Firehose | Data Analytics | Video Streams |\n",
    "|--------------------|--------------|----------------|----------------|----------------|\n",
    "| Purpose            | Ingest data  | Deliver data   | Analyze stream | Analyze video  |\n",
    "| Transformations    | Custom code  | Lambda         | SQL / Flink    | Frame analysis |\n",
    "| Storage            | Temporary    | S3, Redshift   | N/A            | Built-in       |\n",
    "| Latency            | Low          | Medium         | Low            | Varies         |\n",
    "| Complexity         | Medium       | Low            | Medium         | Medium         |\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Common Use Cases\n",
    "\n",
    "| Domain             | Use Case Example                              |\n",
    "|--------------------|-----------------------------------------------|\n",
    "| Analytics          | Real-time dashboards, metrics ingestion       |\n",
    "| Security           | Intrusion detection, log processing           |\n",
    "| IoT                | Sensor data processing                        |\n",
    "| Retail             | Real-time clickstream for personalization     |\n",
    "| Media              | Live video stream processing                  |\n",
    "| Finance            | Trade analysis, fraud detection               |\n",
    "\n",
    "----\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d301bc7",
   "metadata": {},
   "source": [
    "# Spark Structured Streaming\n",
    "\n",
    "## What is Spark Structured Streaming?\n",
    "\n",
    "**Structured Streaming** is a scalable and fault-tolerant stream processing engine built on the **Spark SQL engine**. It allows developers to write streaming jobs the same way they write batch jobs using **DataFrames** and **SQL**.\n",
    "\n",
    "**Key Features:**\n",
    "- Uses **DataFrame API** and **Spark SQL** for stream processing\n",
    "- Supports **exactly-once** semantics\n",
    "- Provides **high-level abstractions** (e.g., windowing, joins, aggregations)\n",
    "- Integrates seamlessly with **batch pipelines**\n",
    "- Supports **event time** processing with **watermarking**\n",
    "\n",
    "Structured Streaming treats a streaming computation as a series of small batch computations, called **micro-batches**.\n",
    "\n",
    "\n",
    "\n",
    "### Stream Reader\n",
    "\n",
    "The **stream reader** defines the source of streaming data.\n",
    "\n",
    "**Common Sources:**\n",
    "- Kafka\n",
    "- File systems (CSV, JSON, Parquet, etc.)\n",
    "- Rate (for testing)\n",
    "- Delta tables\n",
    "\n",
    "**Example:**\n",
    "\n",
    "```python\n",
    "df = spark.readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"localhost:9092\") \\\n",
    "    .option(\"subscribe\", \"topic_name\") \\\n",
    "    .load()\n",
    "````\n",
    "\n",
    "\n",
    "### Stream Writer\n",
    "\n",
    "The **stream writer** defines how the output should be written.\n",
    "\n",
    "**Common Sink Formats:**\n",
    "\n",
    "* console\n",
    "* file (csv, json, parquet, delta)\n",
    "* memory\n",
    "* kafka\n",
    "* table\n",
    "\n",
    "**Example:**\n",
    "\n",
    "```python\n",
    "query = df.writeStream \\\n",
    "    .format(\"console\") \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .start()\n",
    "```\n",
    "\n",
    "**Output Modes:**\n",
    "\n",
    "* `append`: Only new rows\n",
    "* `update`: Updated rows only\n",
    "* `complete`: Full result table (used with aggregations)\n",
    "\n",
    "\n",
    "### Checkpointing\n",
    "\n",
    "**Checkpointing** stores metadata and intermediate state to allow:\n",
    "\n",
    "* Fault tolerance\n",
    "* Stateful processing\n",
    "* Resuming streams on failure\n",
    "\n",
    "**Usage:**\n",
    "\n",
    "```python\n",
    ".writeStream \\\n",
    "    .option(\"checkpointLocation\", \"/path/to/checkpoint\") \\\n",
    "    .start()\n",
    "```\n",
    "\n",
    "Without checkpointing, stateful operations (like aggregations, joins) will fail.\n",
    "\n",
    "\n",
    "## Realtime data processing patterns\n",
    "\n",
    "### Windowing\n",
    "\n",
    "**Windowing** enables time-based groupings for streaming data.\n",
    "\n",
    "**Types of Windows:**\n",
    "\n",
    "* **Tumbling Window**: Fixed-length, non-overlapping\n",
    "* **Sliding Window**: Overlapping windows\n",
    "* **Session Window**: Grouped by inactivity gaps\n",
    "\n",
    "**Example:**\n",
    "\n",
    "```python\n",
    "from pyspark.sql.functions import window\n",
    "\n",
    "df.groupBy(window(\"timestamp\", \"10 minutes\")) \\\n",
    "  .count()\n",
    "```\n",
    "\n",
    "\n",
    "### Watermarking\n",
    "\n",
    "**Watermarking** handles **late-arriving** data in event-time based processing.\n",
    "\n",
    "**Why It's Important:**\n",
    "\n",
    "* Allows system to clean up state\n",
    "* Defines maximum allowed delay for late events\n",
    "\n",
    "**Usage:**\n",
    "\n",
    "```python\n",
    "df.withWatermark(\"event_time\", \"10 minutes\")\n",
    "```\n",
    "\n",
    "Used together with aggregations:\n",
    "\n",
    "```python\n",
    "df.withWatermark(\"event_time\", \"10 minutes\") \\\n",
    "  .groupBy(window(\"event_time\", \"5 minutes\")) \\\n",
    "  .count()\n",
    "```\n",
    "\n",
    "\n",
    "### State Management\n",
    "\n",
    "**Stateful operations** need to track data across micro-batches.\n",
    "\n",
    "**Examples of Stateful Operations:**\n",
    "\n",
    "* Grouped aggregations\n",
    "* Stream-stream joins\n",
    "* Deduplication\n",
    "* Custom state logic via `mapGroupsWithState`\n",
    "\n",
    "**State Store:** Internally stores key-value state, periodically cleaned up using watermarking or timeouts.\n",
    "\n",
    "**Example:**\n",
    "\n",
    "```python\n",
    "df.groupBy(\"id\").count()  # Maintains state per ID\n",
    "```\n",
    "\n",
    "\n",
    "### Stream Processing\n",
    "- **Definition**: Processes each incoming event individually in real-time.\n",
    "- **Tools**: Apache Kafka, Apache Flink, Spark Structured Streaming, AWS Kinesis.\n",
    "- **Use Case**: Fraud detection, stock price tracking.\n",
    "\n",
    "\n",
    "### Micro-Batching\n",
    "- **Definition**: Buffers data for a short duration (e.g., 1–2 seconds) and processes in mini-batches.\n",
    "- **Tools**: Spark Structured Streaming (uses micro-batching internally).\n",
    "- **Use Case**: Real-time log analysis with slight delay tolerance.\n",
    "\n",
    "\n",
    "### Event-Driven Pattern (Lambda Architecture)\n",
    "- **Definition**: Combines batch and stream processing.\n",
    "  - **Speed Layer**: For real-time data.\n",
    "  - **Batch Layer**: For historical data.\n",
    "  - **Serving Layer**: Combines both for querying.\n",
    "- **Use Case**: Real-time + historical analytics dashboards.\n",
    "\n",
    "\n",
    "### Event Sourcing\n",
    "- **Definition**: Records all changes as a sequence of immutable events instead of just the final state.\n",
    "- **Use Case**: Financial systems with full audit trails, systems needing replay capability.\n",
    "\n",
    "### Change Data Capture (CDC)\n",
    "- **Definition**: Captures and streams changes in a database (insert, update, delete).\n",
    "- **Tools**: Debezium, AWS DMS, Oracle GoldenGate.\n",
    "- **Use Case**: Syncing OLTP DB to real-time warehouse.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Summary Table\n",
    "\n",
    "| Concept          | Description                                   | Use Case                           |\n",
    "| ---------------- | --------------------------------------------- | ---------------------------------- |\n",
    "| Stream Reader    | Reads real-time data from source              | Kafka, files, rate source          |\n",
    "| Stream Writer    | Writes results to sink                        | Console, file, delta, memory       |\n",
    "| Checkpointing    | Stores state and progress                     | Required for stateful ops          |\n",
    "| Windowing        | Group data by event time intervals            | Time-based analytics               |\n",
    "| Watermarking     | Handles late data and controls state cleanup  | Event-time aggregations            |\n",
    "| State Management | Maintains intermediate results across batches | Aggregations, joins, deduplication |\n",
    "\n",
    "\n",
    "## Best Practices for Structured Streaming\n",
    "\n",
    "1. **Always Set a Checkpoint Location**\n",
    "\n",
    "   * Required for fault-tolerance and stateful operations.\n",
    "\n",
    "2. **Use Watermarking with Event Time**\n",
    "\n",
    "   * To prevent unbounded state growth and handle late data properly.\n",
    "\n",
    "3. **Optimize Trigger Intervals**\n",
    "\n",
    "   * Use `trigger(processingTime=\"10 seconds\")` to balance latency and throughput.\n",
    "\n",
    "4. **Choose Output Mode Carefully**\n",
    "\n",
    "   * Use `append` for insert-only data.\n",
    "   * Use `update` or `complete` when dealing with aggregations.\n",
    "\n",
    "5. **Use Efficient File Formats**\n",
    "\n",
    "   * Prefer `parquet` or `delta` over `csv/json` for performance.\n",
    "\n",
    "6. **Avoid Overloading Console Sink**\n",
    "\n",
    "   * Console sink is for debugging only, not production.\n",
    "\n",
    "7. **Monitor Streaming Jobs**\n",
    "\n",
    "    * Use Spark UI and logs to track progress, throughput, and state size.\n",
    "\n",
    "\n",
    "```\n",
    "[IoT Devices / App Logs / APIs]\n",
    "        ↓\n",
    "   [Kafka / Kinesis]\n",
    "        ↓\n",
    "[Stream Processor (Flink / Spark / Beam)]\n",
    "        ↓\n",
    "[Realtime DB (Redis / Elastic) + Datalake (S3)]\n",
    "        ↓\n",
    "[Dashboards / Alerts / ML Models]\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10abe0a0",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "#### kinesis-connector for spark streaming\n",
    "\n",
    "```sh\n",
    "sudo wget https://awslabs-code-us-east-1.s3.amazonaws.com/spark-sql-kinesis-connector/spark-streaming-sql-kinesis-connector_2.12-1.2.1.jar\n",
    "sudo chmod 755 spark-streaming-sql-kinesis-connector_2.12-1.2.1.jar\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "12644d95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyspark==3.5.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (3.5.0)\n",
      "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from pyspark==3.5.0) (0.10.9.7)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! pip install pyspark==3.5.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "16c452b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/08/08 04:36:06 WARN Utils: Your hostname, codespaces-c6070e resolves to a loopback address: 127.0.0.1; using 10.0.0.88 instead (on interface eth0)\n",
      "25/08/08 04:36:06 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "25/08/08 04:36:06 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"KinesisConsumer\") \\\n",
    "            .config(\"spark.jars\", \"/workspaces/trng-2286/spark-streaming-sql-kinesis-connector_2.12-1.2.1.jar\")\\\n",
    "            .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c84b7788",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: python-dotenv in /usr/local/python/3.12.1/lib/python3.12/site-packages (1.1.1)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! pip install python-dotenv boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aa0a3de4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "326a9b7d",
   "metadata": {},
   "source": [
    "[aws kinesis docs](https://aws.amazon.com/blogs/big-data/build-spark-structured-streaming-applications-with-the-open-source-connector-for-amazon-kinesis-data-streams/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ce60a3d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "kinesis = spark.readStream.format(\"aws-kinesis\") \\\n",
    "    .option(\"kinesis.region\", \"us-east-1\") \\\n",
    "    .option(\"kinesis.streamName\", \"demo-spark-streaming-events\") \\\n",
    "    .option(\"kinesis.consumerType\", \"GetRecords\") \\\n",
    "    .option(\"kinesis.endpointUrl\", \"https://kinesis.us-east-1.amazonaws.com\") \\\n",
    "    .option(\"kinesis.startingposition\", \"LATEST\") \\\n",
    "    .option(\"kinesis.awsAccessKeyId\", os.getenv(\"AWS_ACCESS_KEY\")) \\\n",
    "    .option(\"kinesis.awsSecretKey\", os.getenv(\"AWS_SECRET_ACCESS_KEY\")) \\\n",
    "    .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "90478e7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"id\", IntegerType()),\n",
    "    StructField(\"event\", StringType()),\n",
    "    StructField(\"value\", DoubleType()),\n",
    "    StructField(\"event_time\", TimestampType())\n",
    "])\n",
    "\n",
    "parsed_df = kinesis \\\n",
    "    .selectExpr(\"CAST(data as STRING) as json_data\") \\\n",
    "    .select(from_json(\"json_data\", schema).alias(\"data\")) \\\n",
    "    .select(\"data.*\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1f3947a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "aggrgated_df = parsed_df \\\n",
    "        .withWatermark(\"event_time\", \"15 minutes\") \\\n",
    "        .groupBy(\n",
    "            window(\"event_time\", \"10 minutes\", \"5 minutes\"), # sliding window\n",
    "            col(\"id\")\n",
    "        ).count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c805fc5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/08/08 04:36:21 WARN GarbageCollectionMetrics: To enable non-built-in garbage collector(s) List(G1 Concurrent GC), users should configure it(them) to spark.eventLog.gcMetrics.youngGenerationGarbageCollectors or spark.eventLog.gcMetrics.oldGenerationGarbageCollectors\n"
     ]
    }
   ],
   "source": [
    "spark.conf.set(\"spark.sql.streaming.forceDeleteTempCheckpointLocation\", True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b994ddd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = parsed_df.writeStream \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .format(\"console\") \\\n",
    "    .option(\"truncate\", \"False\") \\\n",
    "    .option(\"checkpointLocation\", \"kinesis-checkpoint\") \\\n",
    "    .trigger(processingTime = \"10 seconds\") \\\n",
    "    .start()\n",
    "\n",
    "query.awaitTermination()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa95e0f2",
   "metadata": {},
   "source": [
    "```\n",
    "25/08/08 04:27:04 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n",
    "                                                                                \n",
    "-------------------------------------------\n",
    "Batch: 4\n",
    "-------------------------------------------\n",
    "+---+--------+-----+--------------------------+\n",
    "|id |event   |value|event_time                |\n",
    "+---+--------+-----+--------------------------+\n",
    "|359|purchase|43.01|2025-08-08 04:24:48.835473|\n",
    "|953|purchase|95.12|2025-08-08 04:24:50.766439|\n",
    "|67 |click   |10.93|2025-08-08 04:24:51.981412|\n",
    "|667|view    |80.1 |2025-08-08 04:24:53.197143|\n",
    "|265|purchase|70.77|2025-08-08 04:24:54.410997|\n",
    "|363|purchase|51.15|2025-08-08 04:24:55.622517|\n",
    "|840|view    |45.38|2025-08-08 04:24:56.836915|\n",
    "|418|click   |95.0 |2025-08-08 04:24:58.052577|\n",
    "|472|purchase|33.88|2025-08-08 04:24:59.26775 |\n",
    "|702|view    |92.42|2025-08-08 04:25:00.480209|\n",
    "|988|click   |49.43|2025-08-08 04:25:01.693096|\n",
    "|269|click   |62.59|2025-08-08 04:25:02.907686|\n",
    "+---+--------+-----+--------------------------+\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e680425a",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = aggrgated_df.writeStream \\\n",
    "    .outputMode(\"update\") \\\n",
    "    .format(\"console\") \\\n",
    "    .option(\"truncate\", \"False\") \\\n",
    "    .option(\"checkpointLocation\", \"checkpoint\") \\\n",
    "    .trigger(processingTime = \"10 seconds\") \\\n",
    "    .start()\n",
    "\n",
    "query.awaitTermination()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ca9126a",
   "metadata": {},
   "source": [
    "```\n",
    "25/08/08 04:36:52 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n",
    "25/08/08 04:37:18 WARN ProcessingTimeExecutor: Current batch is falling behind. The trigger interval is 10000 milliseconds, but spent 26465 milliseconds\n",
    "-------------------------------------------\n",
    "Batch: 0\n",
    "-------------------------------------------\n",
    "+------+---+-----+\n",
    "|window|id |count|\n",
    "+------+---+-----+\n",
    "+------+---+-----+\n",
    "\n",
    "                                                                                \n",
    "-------------------------------------------\n",
    "Batch: 1\n",
    "-------------------------------------------\n",
    "+------------------------------------------+---+-----+\n",
    "|window                                    |id |count|\n",
    "+------------------------------------------+---+-----+\n",
    "|{2025-08-08 04:35:00, 2025-08-08 04:45:00}|310|1    |\n",
    "|{2025-08-08 04:35:00, 2025-08-08 04:45:00}|640|1    |\n",
    "|{2025-08-08 04:30:00, 2025-08-08 04:40:00}|820|1    |\n",
    "|{2025-08-08 04:35:00, 2025-08-08 04:45:00}|732|1    |\n",
    "|{2025-08-08 04:35:00, 2025-08-08 04:45:00}|309|1    |\n",
    "|{2025-08-08 04:35:00, 2025-08-08 04:45:00}|897|1    |\n",
    "|{2025-08-08 04:30:00, 2025-08-08 04:40:00}|628|1    |\n",
    "|{2025-08-08 04:30:00, 2025-08-08 04:40:00}|309|1    |\n",
    "|{2025-08-08 04:35:00, 2025-08-08 04:45:00}|820|1    |\n",
    "|{2025-08-08 04:35:00, 2025-08-08 04:45:00}|924|1    |\n",
    "|{2025-08-08 04:35:00, 2025-08-08 04:45:00}|974|1    |\n",
    "|{2025-08-08 04:30:00, 2025-08-08 04:40:00}|452|1    |\n",
    "|{2025-08-08 04:35:00, 2025-08-08 04:45:00}|498|1    |\n",
    "|{2025-08-08 04:30:00, 2025-08-08 04:40:00}|507|1    |\n",
    "|{2025-08-08 04:30:00, 2025-08-08 04:40:00}|732|1    |\n",
    "|{2025-08-08 04:30:00, 2025-08-08 04:40:00}|290|1    |\n",
    "|{2025-08-08 04:30:00, 2025-08-08 04:40:00}|20 |1    |\n",
    "|{2025-08-08 04:30:00, 2025-08-08 04:40:00}|498|1    |\n",
    "|{2025-08-08 04:35:00, 2025-08-08 04:45:00}|290|1    |\n",
    "|{2025-08-08 04:30:00, 2025-08-08 04:40:00}|924|1    |\n",
    "+------------------------------------------+---+-----+\n",
    "only showing top 20 rows\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c8c9b32",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = parsed_df.writeStream \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .format(\"csv\") \\\n",
    "    .option(\"path\", \"./datasets/events/\") \\\n",
    "    .option(\"checkpointLocation\", \"kinesis-checkpoint\") \\\n",
    "    .trigger(processingTime = \"10 seconds\") \\\n",
    "    .start()\n",
    "\n",
    "query.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8e138dc2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://3aed53aa-5196-447b-90cf-a7a1b2aecddb.internal.cloudapp.net:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>KinesisConsumer</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7397e5d969c0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b54db190",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
